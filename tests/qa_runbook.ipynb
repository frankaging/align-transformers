{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0c7069a",
   "metadata": {},
   "source": [
    "## Unit Tests and Functional Tests ({Q}uality {A}ssurance) Runbook\n",
    "This is a notebook that shows how we design basic unit / functional tests for our library. Please follow this runbook to contribute more tests to ensure the quality of our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e986e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Zhengxuan Wu\"\n",
    "__version__ = \"12/28/2023\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363cee96",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "We follow generic QA framework, where we write positive and negative test cases for each function or API. For each test, it is better to cover multiple test cases and to have these cases sharing the same set of set-up. In this tutorial, we cover one test case for the subspace intervention with a simple MLP model and static interventions. Overall, we are checking the results based off our API with golden labels that are created using manual interventions. For trainable interventions, we will need to check gradients as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b54a060",
   "metadata": {},
   "source": [
    "### Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becdb6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # This library is our indicator that the required installs\n",
    "    # need to be done.\n",
    "    import transformers\n",
    "    import sys\n",
    "\n",
    "    sys.path.append(\"align-transformers/\")\n",
    "except ModuleNotFoundError:\n",
    "    !git clone https://github.com/frankaging/align-transformers.git\n",
    "    !pip install -r align-transformers/requirements.txt\n",
    "    import sys\n",
    "\n",
    "    sys.path.append(\"align-transformers/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502ae372",
   "metadata": {},
   "source": [
    "### Example of test case for subspace intervention\n",
    "\n",
    "Note that the mindset of developing these test cases should be assuming the code has bug, and want to develop cases where we can maximally trick the system. Here are some tricks in the following test cases:\n",
    "- **Subspace scramble**: We make the order of subspace partition to be unconventional. E,g., we have `[[1,3],[0,1]]` instead of `[[0,1],[1,3]]`. The code should be order agnostic.\n",
    "- **Uneven subspace partition**: instead of having `[[1,3],[0,1]]`, we have an uneven split between subspace.\n",
    "- **Untouched subspace**: Instead of intervening all the subspace avaliable here, we leave one neuron out by having a subspace parition like `[[0,1],[1,2]]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183a1f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import unittest\n",
    "from utils import *\n",
    "\n",
    "\n",
    "class SubspaceInterventionWithMLPTestCase(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(self):\n",
    "        print(\"=== Test Suite: SubspaceInterventionWithMLPTestCase ===\")\n",
    "        self.config, self.tokenizer, self.mlp = create_mlp_classifier(\n",
    "            MLPConfig(h_dim=3, n_layer=1, pdrop=0.0, include_bias=False)\n",
    "        )\n",
    "\n",
    "        self.test_subspace_intervention_link_intervenable_config = IntervenableConfig(\n",
    "            intervenable_model_type=type(self.mlp),\n",
    "            intervenable_representations=[\n",
    "                IntervenableRepresentationConfig(\n",
    "                    0,\n",
    "                    \"mlp_activation\",\n",
    "                    \"pos\",  # mlp layer creates a single token reprs\n",
    "                    1,\n",
    "                    subspace_partition=[\n",
    "                        [1, 3],\n",
    "                        [0, 1],\n",
    "                    ],  # partition into two sets of subspaces\n",
    "                    intervention_link_key=0,  # linked ones target the same subspace\n",
    "                ),\n",
    "                IntervenableRepresentationConfig(\n",
    "                    0,\n",
    "                    \"mlp_activation\",\n",
    "                    \"pos\",  # mlp layer creates a single token reprs\n",
    "                    1,\n",
    "                    subspace_partition=[\n",
    "                        [1, 3],\n",
    "                        [0, 1],\n",
    "                    ],  # partition into two sets of subspaces\n",
    "                    intervention_link_key=0,  # linked ones target the same subspace\n",
    "                ),\n",
    "            ],\n",
    "            intervenable_interventions_type=VanillaIntervention,\n",
    "        )\n",
    "\n",
    "        self.test_subspace_no_intervention_link_intervenable_config = (\n",
    "            IntervenableConfig(\n",
    "                intervenable_model_type=type(self.mlp),\n",
    "                intervenable_representations=[\n",
    "                    IntervenableRepresentationConfig(\n",
    "                        0,\n",
    "                        \"mlp_activation\",\n",
    "                        \"pos\",  # mlp layer creates a single token reprs\n",
    "                        1,\n",
    "                        subspace_partition=[\n",
    "                            [0, 1],\n",
    "                            [1, 3],\n",
    "                        ],  # partition into two sets of subspaces\n",
    "                    ),\n",
    "                    IntervenableRepresentationConfig(\n",
    "                        0,\n",
    "                        \"mlp_activation\",\n",
    "                        \"pos\",  # mlp layer creates a single token reprs\n",
    "                        1,\n",
    "                        subspace_partition=[\n",
    "                            [0, 1],\n",
    "                            [1, 3],\n",
    "                        ],  # partition into two sets of subspaces\n",
    "                    ),\n",
    "                ],\n",
    "                intervenable_interventions_type=VanillaIntervention,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.test_subspace_no_intervention_link_trainable_intervenable_config = (\n",
    "            IntervenableConfig(\n",
    "                intervenable_model_type=type(self.mlp),\n",
    "                intervenable_representations=[\n",
    "                    IntervenableRepresentationConfig(\n",
    "                        0,\n",
    "                        \"mlp_activation\",\n",
    "                        \"pos\",  # mlp layer creates a single token reprs\n",
    "                        1,\n",
    "                        intervenable_low_rank_dimension=2,\n",
    "                        subspace_partition=[\n",
    "                            [0, 1],\n",
    "                            [1, 2],\n",
    "                        ],  # partition into two sets of subspaces\n",
    "                    ),\n",
    "                    IntervenableRepresentationConfig(\n",
    "                        0,\n",
    "                        \"mlp_activation\",\n",
    "                        \"pos\",  # mlp layer creates a single token reprs\n",
    "                        1,\n",
    "                        intervenable_low_rank_dimension=2,\n",
    "                        subspace_partition=[\n",
    "                            [0, 1],\n",
    "                            [1, 2],\n",
    "                        ],  # partition into two sets of subspaces\n",
    "                    ),\n",
    "                ],\n",
    "                intervenable_interventions_type=LowRankRotatedSpaceIntervention,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def test_clean_run_positive(self):\n",
    "        \"\"\"\n",
    "        Positive test case to check whether vanilla forward pass work\n",
    "        with our object.\n",
    "        \"\"\"\n",
    "        intervenable = IntervenableModel(\n",
    "            self.test_subspace_intervention_link_intervenable_config, self.mlp\n",
    "        )\n",
    "        base = {\"inputs_embeds\": torch.rand(10, 1, 3)}\n",
    "        self.assertTrue(\n",
    "            torch.allclose(ONE_MLP_CLEAN_RUN(base, self.mlp), intervenable(base)[0][0])\n",
    "        )\n",
    "\n",
    "    def test_with_subspace_positive(self):\n",
    "        \"\"\"\n",
    "        Positive test case to intervene only a set of subspace.\n",
    "        \"\"\"\n",
    "        intervenable = IntervenableModel(\n",
    "            self.test_subspace_intervention_link_intervenable_config, self.mlp\n",
    "        )\n",
    "        # golden label\n",
    "        b_s = 10\n",
    "        base = {\"inputs_embeds\": torch.rand(b_s, 1, 3)}\n",
    "        source_1 = {\"inputs_embeds\": torch.rand(b_s, 1, 3)}\n",
    "        source_2 = {\"inputs_embeds\": torch.rand(b_s, 1, 3)}\n",
    "        base_act = ONE_MLP_FETCH_W1_ACT(base, self.mlp)\n",
    "        source_1_act = ONE_MLP_FETCH_W1_ACT(source_1, self.mlp)\n",
    "        intervened_act = base_act.clone()  # relentless clone\n",
    "        intervened_act[..., 1:3] = source_1_act[..., 1:3]\n",
    "        golden_out = ONE_MLP_WITH_W1_ACT_RUN(intervened_act, self.mlp)\n",
    "\n",
    "        # our label\n",
    "        _, our_out = intervenable(\n",
    "            base,\n",
    "            [source_1, None],\n",
    "            {\"sources->base\": ([[[0]] * b_s, None], [[[0]] * b_s, None])},\n",
    "            subspaces=[[[0]] * b_s, None],\n",
    "        )\n",
    "        self.assertTrue(torch.allclose(golden_out, our_out[0]))\n",
    "\n",
    "    def test_with_subspace_negative(self):\n",
    "        \"\"\"\n",
    "        Negative test case to check input length.\n",
    "        \"\"\"\n",
    "        intervenable = IntervenableModel(\n",
    "            self.test_subspace_intervention_link_intervenable_config, self.mlp\n",
    "        )\n",
    "        # golden label\n",
    "        b_s = 10\n",
    "        base = {\"inputs_embeds\": torch.rand(b_s, 1, 3)}\n",
    "        source_1 = {\"inputs_embeds\": torch.rand(b_s, 1, 3)}\n",
    "        source_2 = {\"inputs_embeds\": torch.rand(b_s, 1, 3)}\n",
    "\n",
    "        try:\n",
    "            intervenable(\n",
    "                base,\n",
    "                [source_1],\n",
    "                {\"sources->base\": ([[[0]] * b_s], [[[0]] * b_s])},\n",
    "                subspaces=[[[0]] * b_s],\n",
    "            )\n",
    "        except ValueError:\n",
    "            pass\n",
    "        else:\n",
    "            raise AssertionError(\"ValueError was not raised\")\n",
    "\n",
    "    def test_intervention_link_positive(self):\n",
    "        \"\"\"\n",
    "        Positive test case to intervene linked subspace.\n",
    "        \"\"\"\n",
    "        intervenable = IntervenableModel(\n",
    "            self.test_subspace_intervention_link_intervenable_config, self.mlp\n",
    "        )\n",
    "        # golden label\n",
    "        b_s = 10\n",
    "        base = {\"inputs_embeds\": torch.rand(b_s, 1, 3)}\n",
    "        source_1 = {\"inputs_embeds\": torch.rand(b_s, 1, 3)}\n",
    "        source_2 = {\"inputs_embeds\": torch.rand(b_s, 1, 3)}\n",
    "        base_act = ONE_MLP_FETCH_W1_ACT(base, self.mlp)\n",
    "        source_1_act = ONE_MLP_FETCH_W1_ACT(source_1, self.mlp)\n",
    "        source_2_act = ONE_MLP_FETCH_W1_ACT(source_2, self.mlp)\n",
    "\n",
    "        # overwrite version\n",
    "        intervened_act = base_act.clone()  # relentless clone\n",
    "        intervened_act[..., 1:3] = source_2_act[..., 1:3]\n",
    "        golden_out_overwrite = ONE_MLP_WITH_W1_ACT_RUN(intervened_act, self.mlp)\n",
    "\n",
    "        # success version\n",
    "        intervened_act = base_act.clone()  # relentless clone\n",
    "        intervened_act[..., 1:3] = source_1_act[..., 1:3]\n",
    "        intervened_act[..., 0] = source_2_act[..., 0]\n",
    "        golden_out_success = ONE_MLP_WITH_W1_ACT_RUN(intervened_act, self.mlp)\n",
    "\n",
    "        # subcase where the second one accidentally overwrites the first one\n",
    "        _, our_out_overwrite = intervenable(\n",
    "            base,\n",
    "            [source_1, source_2],\n",
    "            {\"sources->base\": ([[[0]] * b_s, [[0]] * b_s], [[[0]] * b_s, [[0]] * b_s])},\n",
    "            subspaces=[[[0]] * b_s, [[0]] * b_s],\n",
    "        )\n",
    "\n",
    "        # success\n",
    "        _, our_out_success = intervenable(\n",
    "            base,\n",
    "            [source_1, source_2],\n",
    "            {\"sources->base\": ([[[0]] * b_s, [[0]] * b_s], [[[0]] * b_s, [[0]] * b_s])},\n",
    "            subspaces=[[[0]] * b_s, [[1]] * b_s],\n",
    "        )\n",
    "\n",
    "        self.assertTrue(torch.allclose(golden_out_overwrite, our_out_overwrite[0]))\n",
    "        self.assertTrue(torch.allclose(golden_out_success, our_out_success[0]))\n",
    "\n",
    "    def test_no_intervention_link_positive(self):\n",
    "        \"\"\"\n",
    "        Positive test case to intervene not linked subspace (overwrite).\n",
    "        \"\"\"\n",
    "        intervenable = IntervenableModel(\n",
    "            self.test_subspace_no_intervention_link_intervenable_config, self.mlp\n",
    "        )\n",
    "        # golden label\n",
    "        b_s = 10\n",
    "        base = {\"inputs_embeds\": torch.rand(b_s, 1, 3)}\n",
    "        source_1 = {\"inputs_embeds\": torch.rand(b_s, 1, 3)}\n",
    "        source_2 = {\"inputs_embeds\": torch.rand(b_s, 1, 3)}\n",
    "        base_act = ONE_MLP_FETCH_W1_ACT(base, self.mlp)\n",
    "        source_1_act = ONE_MLP_FETCH_W1_ACT(source_1, self.mlp)\n",
    "        source_2_act = ONE_MLP_FETCH_W1_ACT(source_2, self.mlp)\n",
    "\n",
    "        # inplace overwrite version\n",
    "        intervened_act = base_act.clone()  # relentless clone\n",
    "        intervened_act[..., 0] = source_2_act[..., 0]\n",
    "        golden_out_inplace = ONE_MLP_WITH_W1_ACT_RUN(intervened_act, self.mlp)\n",
    "\n",
    "        # overwrite version\n",
    "        intervened_act = base_act.clone()  # relentless clone\n",
    "        intervened_act[..., 0] = source_1_act[..., 0]\n",
    "        intervened_act[..., 1:3] = source_2_act[..., 1:3]\n",
    "        golden_out_overwrite = ONE_MLP_WITH_W1_ACT_RUN(intervened_act, self.mlp)\n",
    "\n",
    "        # subcase where the second one accidentally overwrites the first one\n",
    "        _, our_out_inplace = intervenable(\n",
    "            base,\n",
    "            [source_1, source_2],\n",
    "            {\"sources->base\": ([[[0]] * b_s, [[0]] * b_s], [[[0]] * b_s, [[0]] * b_s])},\n",
    "            subspaces=[[[0]] * b_s, [[0]] * b_s],\n",
    "        )\n",
    "\n",
    "        # overwrite\n",
    "        _, our_out_overwrite = intervenable(\n",
    "            base,\n",
    "            [source_1, source_2],\n",
    "            {\"sources->base\": ([[[0]] * b_s, [[0]] * b_s], [[[0]] * b_s, [[0]] * b_s])},\n",
    "            subspaces=[[[0]] * b_s, [[1]] * b_s],\n",
    "        )\n",
    "\n",
    "        self.assertTrue(torch.allclose(golden_out_inplace, our_out_inplace[0]))\n",
    "        # the following thing work but gradient will fail check negative test cases\n",
    "        self.assertTrue(torch.allclose(golden_out_overwrite, our_out_overwrite[0]))\n",
    "\n",
    "    def test_no_intervention_link_negative(self):\n",
    "        pass\n",
    "        \"\"\"\n",
    "        Negative test case to intervene not linked subspace with trainable interventions.\n",
    "        \"\"\"\n",
    "        intervenable = IntervenableModel(\n",
    "            self.test_subspace_no_intervention_link_trainable_intervenable_config,\n",
    "            self.mlp,\n",
    "        )\n",
    "        # golden label\n",
    "        b_s = 10\n",
    "        base = {\"inputs_embeds\": torch.rand(b_s, 1, 3)}\n",
    "        source_1 = {\"inputs_embeds\": torch.rand(b_s, 1, 3)}\n",
    "        source_2 = {\"inputs_embeds\": torch.rand(b_s, 1, 3)}\n",
    "        base_act = ONE_MLP_FETCH_W1_ACT(base, self.mlp)\n",
    "        source_1_act = ONE_MLP_FETCH_W1_ACT(source_1, self.mlp)\n",
    "        source_2_act = ONE_MLP_FETCH_W1_ACT(source_2, self.mlp)\n",
    "\n",
    "        # overwrite version\n",
    "        intervened_act = base_act.clone()  # relentless clone\n",
    "        intervened_act[..., 0] = source_1_act[..., 0]\n",
    "        intervened_act[..., 1] = source_2_act[..., 1]\n",
    "        golden_out_overwrite = ONE_MLP_WITH_W1_ACT_RUN(intervened_act, self.mlp)\n",
    "\n",
    "        # overwrite\n",
    "        _, our_out_overwrite = intervenable(\n",
    "            base,\n",
    "            [source_1, source_2],\n",
    "            {\"sources->base\": ([[[0]] * b_s, [[0]] * b_s], [[[0]] * b_s, [[0]] * b_s])},\n",
    "            subspaces=[[[0]] * b_s, [[1]] * b_s],\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            our_out_overwrite[0].sum().backward()\n",
    "        except RuntimeError:\n",
    "            pass\n",
    "        else:\n",
    "            raise AssertionError(\"RuntimeError by torch was not raised\")\n",
    "\n",
    "\n",
    "def suite():\n",
    "    suite = unittest.TestSuite()\n",
    "    suite.addTest(SubspaceInterventionWithMLPTestCase(\"test_clean_run_positive\"))\n",
    "    suite.addTest(SubspaceInterventionWithMLPTestCase(\"test_with_subspace_positive\"))\n",
    "    suite.addTest(SubspaceInterventionWithMLPTestCase(\"test_with_subspace_negative\"))\n",
    "    suite.addTest(\n",
    "        SubspaceInterventionWithMLPTestCase(\"test_intervention_link_positive\")\n",
    "    )\n",
    "    suite.addTest(\n",
    "        SubspaceInterventionWithMLPTestCase(\"test_no_intervention_link_positive\")\n",
    "    )\n",
    "    suite.addTest(\n",
    "        SubspaceInterventionWithMLPTestCase(\"test_no_intervention_link_negative\")\n",
    "    )\n",
    "    return suite\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    runner = unittest.TextTestRunner()\n",
    "    runner.run(suite())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6708f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
